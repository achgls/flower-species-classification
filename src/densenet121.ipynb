{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"67f32a79-6848-4256-92f9-f781717fe0c9","_uuid":"7196687a-e918-4e96-90cb-693329ee0593","collapsed":false,"execution":{"iopub.execute_input":"2022-11-16T21:59:08.905009Z","iopub.status.busy":"2022-11-16T21:59:08.904557Z","iopub.status.idle":"2022-11-16T21:59:33.826200Z","shell.execute_reply":"2022-11-16T21:59:33.825185Z","shell.execute_reply.started":"2022-11-16T21:59:08.904918Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","from keras_preprocessing.image import ImageDataGenerator\n","import utils\n","import shutil"]},{"cell_type":"markdown","metadata":{},"source":["# Parameters"]},{"cell_type":"markdown","metadata":{},"source":["Using max pooling instead of average pooling could lead to better results as it could help keep the edges crisp and make the leaves pop out from the background, preserving their shape better."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T21:59:33.828962Z","iopub.status.busy":"2022-11-16T21:59:33.828293Z","iopub.status.idle":"2022-11-16T21:59:33.836389Z","shell.execute_reply":"2022-11-16T21:59:33.835555Z","shell.execute_reply.started":"2022-11-16T21:59:33.828923Z"},"trusted":true},"outputs":[],"source":["data_dir = '../input/flowerspeciesclassification/images'\n","model_file_name = 'densenet121'\n","pooling = 'max'  # vs 'avg'\n","batch_size = 128"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T21:59:33.838495Z","iopub.status.busy":"2022-11-16T21:59:33.838032Z","iopub.status.idle":"2022-11-16T21:59:33.857886Z","shell.execute_reply":"2022-11-16T21:59:33.856925Z","shell.execute_reply.started":"2022-11-16T21:59:33.838460Z"},"trusted":true},"outputs":[],"source":["RNG_SEED: int = 7\n","VAL_SPLIT: float = 0.2\n","\n","# Augmented Data Generator parameters\n","# Transformations\n","ROTATION_RANGE: float = 20. # Max rotation in degrees\n","ZOOM_RANGE: float = 0.15 # Max zoom\n","WIDTH_SHIFT_RANGE: int = 10 # Horizontal shift (in pixels)\n","HEIGHT_SHIFT_RANGE: int = 10 # Vertical shift (in pixels)\n","HORIZONTAL_FLIP: bool = True # Horizontal flip\n","VERTICAL_FLIP: bool = False # Vertical flip\n","BRIGHTNESS_SHIFT_RANGE: list = [0.8, 1.2] # Minimum and max brightness scaling\n","SHEAR_RANGE: float = None # Max shear in degrees\n","\n","FILL_MODE: str = \"reflect\" # Filling method for out-of-border pixels"]},{"cell_type":"markdown","metadata":{},"source":["# Create the augmented image data pipe"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T21:59:33.861911Z","iopub.status.busy":"2022-11-16T21:59:33.861278Z","iopub.status.idle":"2022-11-16T21:59:33.869962Z","shell.execute_reply":"2022-11-16T21:59:33.868947Z","shell.execute_reply.started":"2022-11-16T21:59:33.861875Z"},"trusted":true},"outputs":[],"source":["aug_data_gen = ImageDataGenerator(\n","    rotation_range=ROTATION_RANGE,  # Max rotation in degrees\n","    zoom_range=ZOOM_RANGE,  # Max zoom\n","    width_shift_range=WIDTH_SHIFT_RANGE,  # Horizontal shift (in pixels)\n","    height_shift_range=HEIGHT_SHIFT_RANGE,  # Vertical shift (in pixels)\n","    horizontal_flip=HORIZONTAL_FLIP,  # Horizontal flip\n","    vertical_flip=VERTICAL_FLIP,  # Vertical flip\n","    brightness_range=BRIGHTNESS_SHIFT_RANGE,  # Minimum and max brightness scaling\n","    shear_range=SHEAR_RANGE,  # Max shear in degrees\n","    fill_mode=FILL_MODE,  # Filling method for out-of-border pixels\n","    validation_split=VAL_SPLIT\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Train-validation split using ImageDataGenerator object"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T21:59:33.873225Z","iopub.status.busy":"2022-11-16T21:59:33.872302Z","iopub.status.idle":"2022-11-16T21:59:34.460662Z","shell.execute_reply":"2022-11-16T21:59:34.458769Z","shell.execute_reply.started":"2022-11-16T21:59:33.873189Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2836 images belonging to 8 classes.\n","Found 706 images belonging to 8 classes.\n"]}],"source":["aug_train_data = aug_data_gen.flow_from_directory(\n","    directory=data_dir,\n","    target_size=(96,96),\n","    batch_size=batch_size,\n","    subset=\"training\"\n",")\n","\n","aug_val_data = aug_data_gen.flow_from_directory(\n","    directory=data_dir,\n","    target_size=(96,96),\n","    batch_size=batch_size,\n","    subset=\"validation\",\n","    shuffle=False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Build the model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T21:59:34.462512Z","iopub.status.busy":"2022-11-16T21:59:34.462057Z","iopub.status.idle":"2022-11-16T21:59:41.060285Z","shell.execute_reply":"2022-11-16T21:59:41.058392Z","shell.execute_reply.started":"2022-11-16T21:59:34.462474Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-16 21:59:34.588933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:34.672045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:34.672800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:34.673923: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-11-16 21:59:34.674188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:34.674882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:34.675487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:36.696258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:36.697109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:36.697779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-11-16 21:59:36.698424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n","29089792/29084464 [==============================] - 0s 0us/step\n","29097984/29084464 [==============================] - 0s 0us/step\n","Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 96, 96, 3)]       0         \n","_________________________________________________________________\n","tf.math.truediv (TFOpLambda) (None, 96, 96, 3)         0         \n","_________________________________________________________________\n","tf.nn.bias_add (TFOpLambda)  (None, 96, 96, 3)         0         \n","_________________________________________________________________\n","tf.math.truediv_1 (TFOpLambd (None, 96, 96, 3)         0         \n","_________________________________________________________________\n","densenet121 (Functional)     (None, 1024)              7037504   \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               524800    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 8)                 4104      \n","=================================================================\n","Total params: 7,829,064\n","Trainable params: 791,560\n","Non-trainable params: 7,037,504\n","_________________________________________________________________\n"]}],"source":["# Import the base CNN model, keep only the feature-extraction, convolutional part\n","base_model = tf.keras.applications.DenseNet121(\n","    include_top=False,\n","    weights=\"imagenet\",\n","    input_tensor=None,\n","    input_shape=(96, 96, 3),\n","    pooling=pooling,\n","    classes=8,\n",")\n","# Freeze the base CNN model\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","inputs = keras.Input(shape=(96, 96, 3))\n","\n","x = keras.applications.densenet.preprocess_input(inputs)\n","x = base_model(x, training=T)  # set training = False to be in inference mode\n","                                    # i.e not recompute batch norm\n","\n","# --- Feature interpretation / fully-connected part ---\n","# We add 2 intermediate layers, half the size of the feature vector extracted by the CNN\n","# We add dropout as a mean of regularization\n","x = keras.layers.Dense(x.shape[-1]//2, activation='relu')(x)\n","x = keras.layers.Dropout(0.2)(x)\n","\n","x = keras.layers.Dense(x.shape[-1], activation='relu')(x)\n","x = keras.layers.Dropout(0.2)(x)\n","\n","# Set initial biases for classes based on their distribution in the given dataset (not necessary)\n","initial_biases = keras.initializers.Constant(utils.initial_class_biases)\n","\n","# Connect to the output layer with softmax activation\n","outputs = keras.layers.Dense(8, activation=\"softmax\",\n","                            bias_initializer=initial_biases)(x)\n","\n","model = keras.Model(inputs, outputs)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# Training the fully-connected layers (the feature interpretation part)"]},{"cell_type":"markdown","metadata":{},"source":["Let's compile the resulting model with Adam as optimizer function and categorical cross entropy. We make sure to keep track of the accuracy as this is the metric we want to improve for the classification task."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T21:59:41.062477Z","iopub.status.busy":"2022-11-16T21:59:41.062050Z","iopub.status.idle":"2022-11-16T21:59:41.081367Z","shell.execute_reply":"2022-11-16T21:59:41.080398Z","shell.execute_reply.started":"2022-11-16T21:59:41.062439Z"},"trusted":true},"outputs":[],"source":["model.compile(\n","    optimizer=keras.optimizers.Adam(),\n","    loss=keras.losses.CategoricalCrossentropy(),\n","    metrics='accuracy'\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T21:59:41.083400Z","iopub.status.busy":"2022-11-16T21:59:41.082848Z","iopub.status.idle":"2022-11-16T22:13:08.814336Z","shell.execute_reply":"2022-11-16T22:13:08.813346Z","shell.execute_reply.started":"2022-11-16T21:59:41.083362Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-16 21:59:41.866942: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n"]},{"name":"stderr","output_type":"stream","text":["2022-11-16 21:59:48.530074: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"]},{"name":"stdout","output_type":"stream","text":["23/23 [==============================] - 26s 628ms/step - loss: 4.0322 - accuracy: 0.3635 - val_loss: 1.7549 - val_accuracy: 0.5453\n","Epoch 2/200\n","23/23 [==============================] - 14s 589ms/step - loss: 1.8397 - accuracy: 0.5226 - val_loss: 1.2864 - val_accuracy: 0.5878\n","Epoch 3/200\n","23/23 [==============================] - 13s 553ms/step - loss: 1.5442 - accuracy: 0.5716 - val_loss: 1.4643 - val_accuracy: 0.5623\n","Epoch 4/200\n","23/23 [==============================] - 13s 573ms/step - loss: 1.4373 - accuracy: 0.5864 - val_loss: 1.4716 - val_accuracy: 0.5793\n","Epoch 5/200\n","23/23 [==============================] - 13s 547ms/step - loss: 1.5254 - accuracy: 0.5815 - val_loss: 1.6736 - val_accuracy: 0.5057\n","Epoch 6/200\n","23/23 [==============================] - 13s 564ms/step - loss: 1.4449 - accuracy: 0.5913 - val_loss: 1.2871 - val_accuracy: 0.6218\n","Epoch 7/200\n","23/23 [==============================] - 13s 555ms/step - loss: 1.4263 - accuracy: 0.5927 - val_loss: 1.5594 - val_accuracy: 0.5708\n","Epoch 8/200\n","23/23 [==============================] - 13s 565ms/step - loss: 1.3122 - accuracy: 0.6090 - val_loss: 1.4676 - val_accuracy: 0.5807\n","Epoch 9/200\n","23/23 [==============================] - 13s 586ms/step - loss: 1.2988 - accuracy: 0.6104 - val_loss: 1.5609 - val_accuracy: 0.5368\n","Epoch 10/200\n","23/23 [==============================] - 13s 553ms/step - loss: 1.4181 - accuracy: 0.6079 - val_loss: 1.3705 - val_accuracy: 0.5751\n","Epoch 11/200\n","23/23 [==============================] - 13s 577ms/step - loss: 1.2726 - accuracy: 0.6157 - val_loss: 1.2680 - val_accuracy: 0.6275\n","Epoch 12/200\n","23/23 [==============================] - 13s 561ms/step - loss: 1.1721 - accuracy: 0.6587 - val_loss: 1.3872 - val_accuracy: 0.6034\n","Epoch 13/200\n","23/23 [==============================] - 13s 578ms/step - loss: 1.1668 - accuracy: 0.6509 - val_loss: 1.0729 - val_accuracy: 0.6374\n","Epoch 14/200\n","23/23 [==============================] - 13s 570ms/step - loss: 1.1997 - accuracy: 0.6343 - val_loss: 1.3001 - val_accuracy: 0.6275\n","Epoch 15/200\n","23/23 [==============================] - 13s 581ms/step - loss: 1.2177 - accuracy: 0.6315 - val_loss: 1.4702 - val_accuracy: 0.5708\n","Epoch 16/200\n","23/23 [==============================] - 13s 581ms/step - loss: 1.1981 - accuracy: 0.6315 - val_loss: 1.1219 - val_accuracy: 0.6530\n","Epoch 17/200\n","23/23 [==============================] - 13s 588ms/step - loss: 1.0086 - accuracy: 0.6731 - val_loss: 1.2425 - val_accuracy: 0.6218\n","Epoch 18/200\n","23/23 [==============================] - 13s 562ms/step - loss: 1.1167 - accuracy: 0.6460 - val_loss: 1.1478 - val_accuracy: 0.6331\n","Epoch 19/200\n","23/23 [==============================] - 13s 573ms/step - loss: 1.0198 - accuracy: 0.6551 - val_loss: 1.0517 - val_accuracy: 0.6487\n","Epoch 20/200\n","23/23 [==============================] - 13s 552ms/step - loss: 1.0549 - accuracy: 0.6562 - val_loss: 1.2336 - val_accuracy: 0.6317\n","Epoch 21/200\n","23/23 [==============================] - 13s 578ms/step - loss: 1.0417 - accuracy: 0.6601 - val_loss: 1.2207 - val_accuracy: 0.6147\n","Epoch 22/200\n","23/23 [==============================] - 14s 600ms/step - loss: 0.9789 - accuracy: 0.6862 - val_loss: 1.1302 - val_accuracy: 0.6445\n","Epoch 23/200\n","23/23 [==============================] - 13s 585ms/step - loss: 0.9877 - accuracy: 0.6721 - val_loss: 1.0797 - val_accuracy: 0.6544\n","Epoch 24/200\n","23/23 [==============================] - 14s 615ms/step - loss: 0.9616 - accuracy: 0.6837 - val_loss: 1.0727 - val_accuracy: 0.6629\n","Epoch 25/200\n","23/23 [==============================] - 13s 587ms/step - loss: 0.9606 - accuracy: 0.6805 - val_loss: 1.0193 - val_accuracy: 0.6657\n","Epoch 26/200\n","23/23 [==============================] - 13s 587ms/step - loss: 0.9412 - accuracy: 0.6996 - val_loss: 1.1623 - val_accuracy: 0.5878\n","Epoch 27/200\n","23/23 [==============================] - 16s 696ms/step - loss: 0.8765 - accuracy: 0.7038 - val_loss: 1.1923 - val_accuracy: 0.6190\n","Epoch 28/200\n","23/23 [==============================] - 13s 586ms/step - loss: 0.9948 - accuracy: 0.6728 - val_loss: 1.2088 - val_accuracy: 0.6147\n","Epoch 29/200\n","23/23 [==============================] - 13s 587ms/step - loss: 1.0172 - accuracy: 0.6745 - val_loss: 1.0706 - val_accuracy: 0.6558\n","Epoch 30/200\n","23/23 [==============================] - 13s 558ms/step - loss: 0.9556 - accuracy: 0.6932 - val_loss: 1.0759 - val_accuracy: 0.6742\n","Epoch 31/200\n","23/23 [==============================] - 13s 574ms/step - loss: 0.8948 - accuracy: 0.6996 - val_loss: 1.0899 - val_accuracy: 0.6346\n","Epoch 32/200\n","23/23 [==============================] - 13s 581ms/step - loss: 0.9526 - accuracy: 0.6876 - val_loss: 1.1655 - val_accuracy: 0.5963\n","Epoch 33/200\n","23/23 [==============================] - 13s 554ms/step - loss: 0.8686 - accuracy: 0.7070 - val_loss: 1.1336 - val_accuracy: 0.6558\n","Epoch 34/200\n","23/23 [==============================] - 13s 580ms/step - loss: 0.8729 - accuracy: 0.6978 - val_loss: 1.1341 - val_accuracy: 0.6176\n","Epoch 35/200\n","23/23 [==============================] - 13s 549ms/step - loss: 0.8616 - accuracy: 0.7003 - val_loss: 1.1585 - val_accuracy: 0.6388\n","Epoch 36/200\n","23/23 [==============================] - 13s 575ms/step - loss: 0.9196 - accuracy: 0.6908 - val_loss: 1.3809 - val_accuracy: 0.5567\n","Epoch 37/200\n","23/23 [==============================] - 13s 554ms/step - loss: 0.9636 - accuracy: 0.6664 - val_loss: 1.0605 - val_accuracy: 0.6572\n","Epoch 38/200\n","23/23 [==============================] - 13s 563ms/step - loss: 0.8894 - accuracy: 0.6992 - val_loss: 1.0651 - val_accuracy: 0.6289\n","Epoch 39/200\n","23/23 [==============================] - 13s 581ms/step - loss: 0.9044 - accuracy: 0.6904 - val_loss: 1.2030 - val_accuracy: 0.6275\n","Epoch 40/200\n","23/23 [==============================] - 13s 573ms/step - loss: 0.9067 - accuracy: 0.6939 - val_loss: 1.1195 - val_accuracy: 0.6346\n","Epoch 41/200\n","23/23 [==============================] - 13s 579ms/step - loss: 0.8450 - accuracy: 0.7063 - val_loss: 1.0351 - val_accuracy: 0.6374\n","Epoch 42/200\n","23/23 [==============================] - 14s 594ms/step - loss: 0.7756 - accuracy: 0.7317 - val_loss: 1.0637 - val_accuracy: 0.6416\n","Epoch 43/200\n","23/23 [==============================] - 14s 593ms/step - loss: 0.8237 - accuracy: 0.7165 - val_loss: 1.0878 - val_accuracy: 0.6459\n","Epoch 44/200\n","23/23 [==============================] - 13s 549ms/step - loss: 0.8064 - accuracy: 0.7098 - val_loss: 1.0734 - val_accuracy: 0.6558\n","Epoch 45/200\n","23/23 [==============================] - 13s 590ms/step - loss: 0.8775 - accuracy: 0.7003 - val_loss: 1.0929 - val_accuracy: 0.6105\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7feaf00e3a10>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# We give weights for each class to contribute diffrently to the loss function,\n","# such that missed detections on more rare classes will have more impact on the loss function\n","class_weight = {spec: weight for (spec, weight) in zip(range(utils.n_species), utils.class_loss_weights)}\n","\n","model.fit(\n","    aug_train_data,\n","    batch_size=batch_size,\n","    epochs=200,\n","    validation_data=aug_val_data,\n","    class_weight=class_weight,\n","    callbacks = [\n","        keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True),\n","        keras.callbacks.TensorBoard(\n","            log_dir=\"tensorboard_logs\", \n","            profile_batch=0,\n","            histogram_freq=1\n","        )  # if > 0 (epochs) shows weights histograms\n","    ]\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T22:13:08.816410Z","iopub.status.busy":"2022-11-16T22:13:08.815773Z","iopub.status.idle":"2022-11-16T22:14:12.096222Z","shell.execute_reply":"2022-11-16T22:14:12.095170Z","shell.execute_reply.started":"2022-11-16T22:13:08.816374Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-16 22:13:35.467008: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"]}],"source":["model.save(model_file_name + \"_frozen\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T22:14:12.113103Z","iopub.status.busy":"2022-11-16T22:14:12.112744Z","iopub.status.idle":"2022-11-16T22:14:14.320417Z","shell.execute_reply":"2022-11-16T22:14:14.319471Z","shell.execute_reply.started":"2022-11-16T22:14:12.113073Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/densenet121_noupscaling_frozen.zip'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["shutil.make_archive(model_file_name + \"_frozen\", 'zip', model_file_name + \"_frozen\")"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning the full model including feature-extraction"]},{"cell_type":"markdown","metadata":{},"source":["Unfreeze the convolutive model and recompile the total model. We will train again using a much lower learning rate in order to fine-tune the weights in the feature-extraction part of the model, i.e the `DenseNet121`."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T22:14:14.322364Z","iopub.status.busy":"2022-11-16T22:14:14.321971Z","iopub.status.idle":"2022-11-16T22:32:28.411796Z","shell.execute_reply":"2022-11-16T22:32:28.410864Z","shell.execute_reply.started":"2022-11-16T22:14:14.322328Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","23/23 [==============================] - 31s 729ms/step - loss: 1.5961 - accuracy: 0.4905 - val_loss: 1.0984 - val_accuracy: 0.5680\n","Epoch 2/200\n","23/23 [==============================] - 14s 614ms/step - loss: 0.9960 - accuracy: 0.6492 - val_loss: 0.9315 - val_accuracy: 0.6388\n","Epoch 3/200\n","23/23 [==============================] - 14s 621ms/step - loss: 0.8330 - accuracy: 0.6999 - val_loss: 0.7997 - val_accuracy: 0.7252\n","Epoch 4/200\n","23/23 [==============================] - 14s 617ms/step - loss: 0.7262 - accuracy: 0.7320 - val_loss: 0.8358 - val_accuracy: 0.6912\n","Epoch 5/200\n","23/23 [==============================] - 15s 622ms/step - loss: 0.6452 - accuracy: 0.7669 - val_loss: 0.6951 - val_accuracy: 0.7394\n","Epoch 6/200\n","23/23 [==============================] - 14s 627ms/step - loss: 0.6021 - accuracy: 0.7898 - val_loss: 0.8279 - val_accuracy: 0.6898\n","Epoch 7/200\n","23/23 [==============================] - 14s 609ms/step - loss: 0.5791 - accuracy: 0.7898 - val_loss: 0.6760 - val_accuracy: 0.7762\n","Epoch 8/200\n","23/23 [==============================] - 15s 641ms/step - loss: 0.4615 - accuracy: 0.8406 - val_loss: 0.5846 - val_accuracy: 0.7861\n","Epoch 9/200\n","23/23 [==============================] - 14s 615ms/step - loss: 0.4767 - accuracy: 0.8322 - val_loss: 0.7194 - val_accuracy: 0.7465\n","Epoch 10/200\n","23/23 [==============================] - 14s 624ms/step - loss: 0.4015 - accuracy: 0.8526 - val_loss: 0.5203 - val_accuracy: 0.8187\n","Epoch 11/200\n","23/23 [==============================] - 15s 636ms/step - loss: 0.3873 - accuracy: 0.8692 - val_loss: 0.5548 - val_accuracy: 0.8229\n","Epoch 12/200\n","23/23 [==============================] - 14s 614ms/step - loss: 0.3603 - accuracy: 0.8664 - val_loss: 0.6747 - val_accuracy: 0.7720\n","Epoch 13/200\n","23/23 [==============================] - 14s 620ms/step - loss: 0.3448 - accuracy: 0.8745 - val_loss: 0.5794 - val_accuracy: 0.7946\n","Epoch 14/200\n","23/23 [==============================] - 14s 620ms/step - loss: 0.2987 - accuracy: 0.8967 - val_loss: 0.6707 - val_accuracy: 0.7790\n","Epoch 15/200\n","23/23 [==============================] - 14s 622ms/step - loss: 0.3152 - accuracy: 0.8745 - val_loss: 0.6135 - val_accuracy: 0.8031\n","Epoch 16/200\n","23/23 [==============================] - 14s 612ms/step - loss: 0.2894 - accuracy: 0.8939 - val_loss: 0.7767 - val_accuracy: 0.7535\n","Epoch 17/200\n","23/23 [==============================] - 14s 616ms/step - loss: 0.2530 - accuracy: 0.9037 - val_loss: 0.5337 - val_accuracy: 0.8357\n","Epoch 18/200\n","23/23 [==============================] - 15s 634ms/step - loss: 0.2640 - accuracy: 0.9044 - val_loss: 0.5311 - val_accuracy: 0.8272\n","Epoch 19/200\n","23/23 [==============================] - 14s 610ms/step - loss: 0.3018 - accuracy: 0.8921 - val_loss: 0.6815 - val_accuracy: 0.7847\n","Epoch 20/200\n","23/23 [==============================] - 15s 634ms/step - loss: 0.2268 - accuracy: 0.9168 - val_loss: 0.6334 - val_accuracy: 0.8300\n","Epoch 21/200\n","23/23 [==============================] - 15s 636ms/step - loss: 0.2651 - accuracy: 0.9126 - val_loss: 0.6956 - val_accuracy: 0.7762\n","Epoch 22/200\n","23/23 [==============================] - 14s 607ms/step - loss: 0.2964 - accuracy: 0.8995 - val_loss: 0.5813 - val_accuracy: 0.8399\n","Epoch 23/200\n","23/23 [==============================] - 14s 617ms/step - loss: 0.2067 - accuracy: 0.9263 - val_loss: 0.5216 - val_accuracy: 0.8258\n","Epoch 24/200\n","23/23 [==============================] - 14s 613ms/step - loss: 0.1486 - accuracy: 0.9443 - val_loss: 0.5245 - val_accuracy: 0.8584\n","Epoch 25/200\n","23/23 [==============================] - 14s 625ms/step - loss: 0.1967 - accuracy: 0.9281 - val_loss: 0.5900 - val_accuracy: 0.8244\n","Epoch 26/200\n","23/23 [==============================] - 14s 612ms/step - loss: 0.1782 - accuracy: 0.9334 - val_loss: 0.5057 - val_accuracy: 0.8584\n","Epoch 27/200\n","23/23 [==============================] - 14s 600ms/step - loss: 0.1344 - accuracy: 0.9528 - val_loss: 0.5873 - val_accuracy: 0.8428\n","Epoch 28/200\n","23/23 [==============================] - 14s 604ms/step - loss: 0.1368 - accuracy: 0.9464 - val_loss: 0.5483 - val_accuracy: 0.8569\n","Epoch 29/200\n","23/23 [==============================] - 14s 609ms/step - loss: 0.1190 - accuracy: 0.9545 - val_loss: 0.5325 - val_accuracy: 0.8470\n","Epoch 30/200\n","23/23 [==============================] - 14s 609ms/step - loss: 0.0986 - accuracy: 0.9630 - val_loss: 0.5435 - val_accuracy: 0.8654\n","Epoch 31/200\n","23/23 [==============================] - 14s 607ms/step - loss: 0.1000 - accuracy: 0.9619 - val_loss: 0.5714 - val_accuracy: 0.8385\n","Epoch 32/200\n","23/23 [==============================] - 14s 604ms/step - loss: 0.0980 - accuracy: 0.9665 - val_loss: 0.6648 - val_accuracy: 0.8258\n","Epoch 33/200\n","23/23 [==============================] - 14s 607ms/step - loss: 0.1254 - accuracy: 0.9559 - val_loss: 0.7128 - val_accuracy: 0.8159\n","Epoch 34/200\n","23/23 [==============================] - 14s 605ms/step - loss: 0.0849 - accuracy: 0.9654 - val_loss: 0.5393 - val_accuracy: 0.8569\n","Epoch 35/200\n","23/23 [==============================] - 14s 598ms/step - loss: 0.0622 - accuracy: 0.9778 - val_loss: 0.8096 - val_accuracy: 0.8428\n","Epoch 36/200\n","23/23 [==============================] - 15s 629ms/step - loss: 0.1387 - accuracy: 0.9503 - val_loss: 0.4344 - val_accuracy: 0.8810\n","Epoch 37/200\n","23/23 [==============================] - 15s 635ms/step - loss: 0.1334 - accuracy: 0.9535 - val_loss: 0.5005 - val_accuracy: 0.8640\n","Epoch 38/200\n","23/23 [==============================] - 14s 603ms/step - loss: 0.0943 - accuracy: 0.9644 - val_loss: 0.5636 - val_accuracy: 0.8569\n","Epoch 39/200\n","23/23 [==============================] - 14s 626ms/step - loss: 0.1616 - accuracy: 0.9397 - val_loss: 0.6679 - val_accuracy: 0.8314\n","Epoch 40/200\n","23/23 [==============================] - 14s 618ms/step - loss: 0.1280 - accuracy: 0.9524 - val_loss: 0.5786 - val_accuracy: 0.8541\n","Epoch 41/200\n","23/23 [==============================] - 15s 651ms/step - loss: 0.1369 - accuracy: 0.9517 - val_loss: 0.6481 - val_accuracy: 0.8343\n","Epoch 42/200\n","23/23 [==============================] - 15s 621ms/step - loss: 0.1058 - accuracy: 0.9616 - val_loss: 0.6105 - val_accuracy: 0.8555\n","Epoch 43/200\n","23/23 [==============================] - 15s 657ms/step - loss: 0.0785 - accuracy: 0.9697 - val_loss: 0.8207 - val_accuracy: 0.8017\n","Epoch 44/200\n","23/23 [==============================] - 14s 601ms/step - loss: 0.1693 - accuracy: 0.9397 - val_loss: 0.6086 - val_accuracy: 0.8272\n","Epoch 45/200\n","23/23 [==============================] - 15s 634ms/step - loss: 0.0864 - accuracy: 0.9697 - val_loss: 0.5047 - val_accuracy: 0.8768\n","Epoch 46/200\n","23/23 [==============================] - 14s 610ms/step - loss: 0.0494 - accuracy: 0.9810 - val_loss: 0.5714 - val_accuracy: 0.8739\n","Epoch 47/200\n","23/23 [==============================] - 14s 604ms/step - loss: 0.1057 - accuracy: 0.9640 - val_loss: 0.8320 - val_accuracy: 0.8059\n","Epoch 48/200\n","23/23 [==============================] - 14s 621ms/step - loss: 0.1069 - accuracy: 0.9605 - val_loss: 0.5636 - val_accuracy: 0.8527\n","Epoch 49/200\n","23/23 [==============================] - 14s 607ms/step - loss: 0.0651 - accuracy: 0.9743 - val_loss: 0.6193 - val_accuracy: 0.8598\n","Epoch 50/200\n","23/23 [==============================] - 15s 639ms/step - loss: 0.0708 - accuracy: 0.9788 - val_loss: 0.5652 - val_accuracy: 0.8640\n","Epoch 51/200\n","23/23 [==============================] - 14s 593ms/step - loss: 0.1050 - accuracy: 0.9686 - val_loss: 0.6323 - val_accuracy: 0.8286\n","Epoch 52/200\n","23/23 [==============================] - 14s 630ms/step - loss: 0.1263 - accuracy: 0.9528 - val_loss: 0.6223 - val_accuracy: 0.8484\n","Epoch 53/200\n","23/23 [==============================] - 14s 620ms/step - loss: 0.0927 - accuracy: 0.9679 - val_loss: 0.5881 - val_accuracy: 0.8499\n","Epoch 54/200\n","23/23 [==============================] - 14s 596ms/step - loss: 0.0457 - accuracy: 0.9820 - val_loss: 0.5542 - val_accuracy: 0.8626\n","Epoch 55/200\n","23/23 [==============================] - 17s 726ms/step - loss: 0.0978 - accuracy: 0.9679 - val_loss: 0.6320 - val_accuracy: 0.8527\n","Epoch 56/200\n","23/23 [==============================] - 14s 617ms/step - loss: 0.0826 - accuracy: 0.9700 - val_loss: 0.6155 - val_accuracy: 0.8470\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fe688286ad0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Unfreeze the base CNN model\n","for layer in base_model.layers:\n","    layer.trainable = True\n","\n","model.compile(\n","    optimizer=keras.optimizers.Adam(1e-4),  # or even 1e-6, low learning rate is necessary for fine-tuning\n","    loss=keras.losses.CategoricalCrossentropy(),\n","    metrics='accuracy'\n",")\n","\n","model.fit(\n","    aug_train_data,\n","    batch_size=batch_size,\n","    epochs=200,\n","    validation_data=aug_val_data,\n","    class_weight=class_weight,\n","    callbacks = [\n","        keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True),\n","        keras.callbacks.TensorBoard(\n","            log_dir=\"tensorboard_logs\", \n","            profile_batch=0,\n","            histogram_freq=1\n","        )  # if > 0 (epochs) shows weights histograms\n","    ]\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T22:32:28.414879Z","iopub.status.busy":"2022-11-16T22:32:28.414551Z","iopub.status.idle":"2022-11-16T22:33:31.129042Z","shell.execute_reply":"2022-11-16T22:33:31.128028Z","shell.execute_reply.started":"2022-11-16T22:32:28.414851Z"},"trusted":true},"outputs":[],"source":["model.save(model_file_name)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T22:33:31.145101Z","iopub.status.busy":"2022-11-16T22:33:31.144791Z","iopub.status.idle":"2022-11-16T22:33:36.062143Z","shell.execute_reply":"2022-11-16T22:33:36.061234Z","shell.execute_reply.started":"2022-11-16T22:33:31.145074Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/densenet121_noupscaling.zip'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["shutil.make_archive(model_file_name, 'zip', model_file_name)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-11-16T22:33:36.065696Z","iopub.status.busy":"2022-11-16T22:33:36.065394Z","iopub.status.idle":"2022-11-16T22:33:53.299040Z","shell.execute_reply":"2022-11-16T22:33:53.298066Z","shell.execute_reply.started":"2022-11-16T22:33:36.065670Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/tensorboard_logs_densenet121_noupscaling.zip'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["shutil.make_archive(\"tensorboard_logs_\" + model_file_name, 'zip', \"tensorboard_logs\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('AN2DL')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"694ab38481603c2df9c61672da44e862d6f0c2606ec9afdaa1a88c85b2dd93ea"}}},"nbformat":4,"nbformat_minor":4}
